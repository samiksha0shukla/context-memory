# FAISS Vector Database Integration Guide for ContextMemory

A complete, step-by-step guide for integrating FAISS vector database into ContextMemory to achieve ~100x faster similarity search.

---

## Table of Contents
1. [What is FAISS?](#1-what-is-faiss)
2. [Why FAISS for ContextMemory?](#2-why-faiss-for-contextmemory)
3. [How FAISS Works](#3-how-faiss-works)
4. [Before vs After](#4-before-vs-after)
5. [Step-by-Step Implementation](#5-step-by-step-implementation)
6. [Complete Code Files](#6-complete-code-files)
7. [Testing the Integration](#7-testing-the-integration)
8. [Troubleshooting](#8-troubleshooting)

---

## 1. What is FAISS?

**FAISS** (Facebook AI Similarity Search) is a library for efficient similarity search of dense vectors.

### Simple Analogy
Imagine you have a library with 10,000 books. To find a similar book:
- **Without FAISS**: Read every book (slow!)
- **With FAISS**: Use an organized index to jump directly to similar books (fast!)

### Key Concepts

| Term | Meaning |
|------|---------|
| **Vector** | A list of numbers representing text (e.g., `[0.1, 0.2, ..., 0.9]`) |
| **Embedding** | The vector representation of text, generated by OpenAI |
| **Index** | A data structure that organizes vectors for fast search |
| **Dimension** | Length of vectors (OpenAI's `text-embedding-3-small` = 1536) |
| **k** | Number of results to return |

---

## 2. Why FAISS for ContextMemory?

### Current Problem
```
User sends message → Generate embedding → Fetch ALL memories → 
Loop through each → Calculate similarity manually → Sort → Return top 10

Time: O(n) = 500ms-2000ms for 10,000 memories
```

### With FAISS
```
User sends message → Generate embedding → FAISS search (indexed) → Return top 10

Time: O(log n) = 1-10ms for 10,000 memories
```

### Speed Comparison

| Memories | Current (O(n)) | With FAISS |
|----------|----------------|------------|
| 100 | ~50ms | ~1ms |
| 1,000 | ~200ms | ~2ms |
| 10,000 | ~1000ms | ~5ms |
| 100,000 | ~10,000ms | ~10ms |

---

## 3. How FAISS Works

### Step 1: Create Index
```python
import faiss

# Create an index for 1536-dimensional vectors
index = faiss.IndexFlatIP(1536)  # IP = Inner Product (cosine similarity)
```

### Step 2: Add Vectors
```python
import numpy as np

# Convert embedding to numpy array
vector = np.array([[0.1, 0.2, 0.3, ...]], dtype=np.float32)

# Normalize for cosine similarity
faiss.normalize_L2(vector)

# Add to index
index.add(vector)
```

### Step 3: Search
```python
# Search for top 10 similar vectors
query = np.array([[0.15, 0.22, 0.28, ...]], dtype=np.float32)
faiss.normalize_L2(query)

scores, indices = index.search(query, k=10)
# scores: similarity scores
# indices: positions in the index
```

### Step 4: Persist to Disk
```python
# Save
faiss.write_index(index, "my_index.faiss")

# Load
index = faiss.read_index("my_index.faiss")
```

---

## 4. Before vs After

### Before (Current Code)

**File: `src/contextmemory/memory/similar_memory_search.py`**
```python
def search_similar_memories(db, conversation_id, query_embeddings, limit=10):
    # Fetch ALL memories (slow with many memories)
    memories = db.query(Memory).filter(...).all()
    
    # Loop through each one (O(n))
    scored = []
    for mem in memories:
        score = cosine_similarity(query_embeddings, mem.embedding)
        scored.append((score, mem))
    
    # Sort manually
    scored.sort(key=lambda x: x[0], reverse=True)
    return [mem for _, mem in scored[:limit]]
```

### After (With FAISS)

```python
def search_similar_memories(db, conversation_id, query_embeddings, limit=10):
    # Get FAISS index (O(1))
    vector_store = get_vector_store(conversation_id)
    
    # Search (O(log n))
    results = vector_store.search(query_embeddings, k=limit)
    
    # Fetch only the matching memory objects
    memory_ids = [r["memory_id"] for r in results]
    memories = db.query(Memory).filter(Memory.id.in_(memory_ids)).all()
    
    # Maintain order
    id_to_mem = {m.id: m for m in memories}
    return [id_to_mem[mid] for mid in memory_ids if mid in id_to_mem]
```

---

## 5. Step-by-Step Implementation

### Step 1: Install FAISS

```bash
cd /Users/samikshashukla/Documents/context-memory
source venv/bin/activate
pip install faiss-cpu numpy
```

### Step 2: Update `pyproject.toml`

**File:** `pyproject.toml`  
**Location:** Lines 37-43

```toml
dependencies = [
    "openai>=2.0.0",
    "sqlalchemy>=2.0.0",
    "pydantic>=2.0.0",
    "psycopg2-binary>=2.9.0",
    "python-dotenv>=1.0.0",
    "faiss-cpu>=1.7.0",      # ADD THIS
    "numpy>=1.24.0",          # ADD THIS
]
```

### Step 3: Create Vector Store Module

**File:** `src/contextmemory/memory/vector_store.py` (NEW FILE)

```python
"""
FAISS Vector Store for ContextMemory.

This module provides fast vector similarity search using FAISS.
Each conversation has its own index for isolation.
"""

import faiss
import numpy as np
from typing import List, Dict, Optional
import os
import json


class FAISSVectorStore:
    """
    A FAISS-backed vector store for fast similarity search.
    
    Attributes:
        dimension: The size of embedding vectors (1536 for OpenAI)
        index: The FAISS index
        id_map: Maps memory_id -> faiss_index
        reverse_map: Maps faiss_index -> memory_id
    """
    
    def __init__(self, dimension: int = 1536):
        """
        Initialize a new vector store.
        
        Args:
            dimension: Size of vectors (default 1536 for OpenAI embeddings)
        """
        self.dimension = dimension
        
        # IndexFlatIP = Inner Product (cosine similarity after normalization)
        self.index = faiss.IndexFlatIP(dimension)
        
        # Bidirectional mapping between memory IDs and FAISS indices
        self.id_map: Dict[int, int] = {}  # memory_id -> faiss_index
        self.reverse_map: Dict[int, int] = {}  # faiss_index -> memory_id
    
    def add(self, memory_id: int, embedding: List[float]) -> None:
        """
        Add a memory embedding to the index.
        
        Args:
            memory_id: The database ID of the memory
            embedding: The 1536-dimensional embedding vector
        """
        # Convert to numpy array with correct shape
        vector = np.array([embedding], dtype=np.float32)
        
        # Normalize for cosine similarity
        # After normalization, inner product = cosine similarity
        faiss.normalize_L2(vector)
        
        # Get the index position before adding
        faiss_idx = self.index.ntotal
        
        # Add to FAISS
        self.index.add(vector)
        
        # Update mappings
        self.id_map[memory_id] = faiss_idx
        self.reverse_map[faiss_idx] = memory_id
    
    def search(self, query_embedding: List[float], k: int = 10) -> List[Dict]:
        """
        Search for similar vectors.
        
        Args:
            query_embedding: The query vector
            k: Number of results to return
            
        Returns:
            List of dicts with memory_id and score
        """
        if self.index.ntotal == 0:
            return []
        
        # Prepare query vector
        vector = np.array([query_embedding], dtype=np.float32)
        faiss.normalize_L2(vector)
        
        # Don't request more than we have
        k = min(k, self.index.ntotal)
        
        # Search
        scores, indices = self.index.search(vector, k)
        
        # Map back to memory IDs
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx != -1 and idx in self.reverse_map:
                results.append({
                    "memory_id": self.reverse_map[idx],
                    "score": float(score)
                })
        
        return results
    
    def remove(self, memory_id: int) -> None:
        """
        Remove a memory from tracking (soft delete).
        
        Note: FAISS doesn't support true deletion. The vector remains
        in the index but won't be returned in results.
        """
        if memory_id in self.id_map:
            faiss_idx = self.id_map[memory_id]
            del self.reverse_map[faiss_idx]
            del self.id_map[memory_id]
    
    def save(self, path: str) -> None:
        """
        Save index and mappings to disk.
        
        Args:
            path: Base path (without extension)
        """
        os.makedirs(os.path.dirname(path), exist_ok=True)
        faiss.write_index(self.index, f"{path}.faiss")
        with open(f"{path}.map.json", "w") as f:
            json.dump({
                "id_map": {str(k): v for k, v in self.id_map.items()},
                "reverse_map": {str(k): v for k, v in self.reverse_map.items()}
            }, f)
    
    def load(self, path: str) -> bool:
        """
        Load index and mappings from disk.
        
        Args:
            path: Base path (without extension)
            
        Returns:
            True if loaded successfully, False if files don't exist
        """
        if not os.path.exists(f"{path}.faiss"):
            return False
        
        self.index = faiss.read_index(f"{path}.faiss")
        with open(f"{path}.map.json", "r") as f:
            data = json.load(f)
            self.id_map = {int(k): v for k, v in data["id_map"].items()}
            self.reverse_map = {int(k): v for k, v in data["reverse_map"].items()}
        return True
    
    @property
    def count(self) -> int:
        """Number of vectors in the index."""
        return len(self.id_map)


# Global cache of vector stores (one per conversation)
_vector_stores: Dict[int, FAISSVectorStore] = {}


def get_index_path(conversation_id: int) -> str:
    """Get the file path for a conversation's index."""
    index_dir = os.path.expanduser("~/.contextmemory/indexes")
    os.makedirs(index_dir, exist_ok=True)
    return os.path.join(index_dir, f"conv_{conversation_id}")


def get_vector_store(conversation_id: int) -> FAISSVectorStore:
    """
    Get or create a vector store for a conversation.
    
    This is the main entry point for using FAISS in ContextMemory.
    
    Args:
        conversation_id: The conversation to get the store for
        
    Returns:
        FAISSVectorStore instance
    """
    global _vector_stores
    
    if conversation_id not in _vector_stores:
        store = FAISSVectorStore()
        path = get_index_path(conversation_id)
        store.load(path)  # Load if exists, otherwise empty
        _vector_stores[conversation_id] = store
    
    return _vector_stores[conversation_id]


def save_vector_store(conversation_id: int) -> None:
    """Save a conversation's vector store to disk."""
    if conversation_id in _vector_stores:
        path = get_index_path(conversation_id)
        _vector_stores[conversation_id].save(path)


def rebuild_index_from_db(db, conversation_id: int) -> FAISSVectorStore:
    """
    Rebuild FAISS index from database.
    
    Use this when:
    - Index file is missing or corrupted
    - After bulk operations
    - For initial migration
    
    Args:
        db: SQLAlchemy session
        conversation_id: Conversation to rebuild
        
    Returns:
        New FAISSVectorStore with all memories indexed
    """
    from contextmemory.db.models.memory import Memory
    
    store = FAISSVectorStore()
    
    # Fetch all memories with embeddings
    memories = db.query(Memory).filter(
        Memory.conversation_id == conversation_id,
        Memory.is_active == True,
        Memory.embedding.isnot(None)
    ).all()
    
    # Add each to the index
    for mem in memories:
        if mem.embedding:
            store.add(mem.id, mem.embedding)
    
    # Cache and save
    _vector_stores[conversation_id] = store
    save_vector_store(conversation_id)
    
    return store
```

### Step 4: Update Similar Memory Search

**File:** `src/contextmemory/memory/similar_memory_search.py`

**Replace entire file with:**

```python
"""
Similar Memory Search using FAISS.

This module finds memories similar to a query using vector search.
"""

from typing import List
from sqlalchemy.orm import Session

from contextmemory.db.models.memory import Memory
from contextmemory.memory.vector_store import get_vector_store, rebuild_index_from_db


def search_similar_memories(
    db: Session, 
    conversation_id: int, 
    query_embeddings: List[float], 
    limit: int = 10
) -> List[Memory]:
    """
    Find memories similar to the query embedding.
    
    Uses FAISS for O(log n) search instead of O(n) brute force.
    
    Args:
        db: Database session
        conversation_id: Conversation to search in
        query_embeddings: Query vector (1536 dimensions)
        limit: Max results to return
        
    Returns:
        List of Memory objects, ordered by similarity
    """
    # Get or create FAISS index
    vector_store = get_vector_store(conversation_id)
    
    # If index is empty, try rebuilding from DB
    if vector_store.count == 0:
        vector_store = rebuild_index_from_db(db, conversation_id)
    
    # Search FAISS (O(log n))
    results = vector_store.search(query_embeddings, k=limit)
    
    if not results:
        return []
    
    # Fetch Memory objects by IDs
    memory_ids = [r["memory_id"] for r in results]
    memories = db.query(Memory).filter(Memory.id.in_(memory_ids)).all()
    
    # Maintain similarity order
    id_to_mem = {m.id: m for m in memories}
    ordered = [id_to_mem[mid] for mid in memory_ids if mid in id_to_mem]
    
    return ordered
```

### Step 5: Update Memory.py search()

**File:** `src/contextmemory/memory/memory.py`  
**Function:** `search()` (around line 70)

**Replace the search method with:**

```python
def search(self, query: str, conversation_id: int, limit: int = 10, include_connections: bool = True) -> Dict:
    """
    Search for relevant memories using FAISS.
    
    Args:
        query: Search query text
        conversation_id: Conversation to search
        limit: Max results
        include_connections: Include connected bubbles
        
    Returns:
        Dict with query and results
    """
    from contextmemory.memory.vector_store import get_vector_store, rebuild_index_from_db
    
    # Generate query embedding
    query_embedding = embed_text(query)
    
    # Get FAISS index
    vector_store = get_vector_store(conversation_id)
    
    # Rebuild if empty
    if vector_store.count == 0:
        vector_store = rebuild_index_from_db(self.db, conversation_id)
    
    # FAISS search (O(log n))
    faiss_results = vector_store.search(query_embedding, k=limit * 2)
    
    if not faiss_results:
        return {"query": query, "results": []}
    
    # Fetch Memory objects
    memory_ids = [r["memory_id"] for r in faiss_results]
    memories = self.db.query(Memory).filter(
        Memory.id.in_(memory_ids),
        Memory.is_active == True
    ).all()
    
    # Create lookup
    id_to_mem = {m.id: m for m in memories}
    faiss_scores = {r["memory_id"]: r["score"] for r in faiss_results}
    
    # Score with recency and importance
    now = datetime.now(timezone.utc)
    scored = []
    
    for mem in memories:
        similarity = faiss_scores.get(mem.id, 0)
        
        # Recency decay for bubbles
        if mem.is_episodic and mem.occurred_at:
            days_ago = (now - mem.occurred_at).days
            recency = math.exp(-0.05 * days_ago)
        else:
            recency = 1.0
        
        importance = mem.importance if mem.importance else 0.5
        final_score = similarity * importance * recency
        scored.append((final_score, mem))
    
    # Sort and limit
    scored.sort(key=lambda x: x[0], reverse=True)
    top_results = scored[:limit]
    
    # Collect connected bubbles
    result_ids = {mem.id for _, mem in top_results}
    connected = []
    
    if include_connections:
        for _, mem in top_results:
            if mem.memory_metadata and "connections" in mem.memory_metadata:
                conn_ids = mem.memory_metadata["connections"].get("bubble_ids", [])
                for conn_id in conn_ids[:2]:
                    if conn_id not in result_ids:
                        conn_mem = self.db.get(Memory, conn_id)
                        if conn_mem and conn_mem.is_active:
                            connected.append(conn_mem)
                            result_ids.add(conn_id)
    
    # Format results
    results = []
    for score, mem in top_results:
        results.append({
            "memory_id": mem.id,
            "memory": mem.memory_text,
            "type": "bubble" if mem.is_episodic else "semantic",
            "occurred_at": mem.occurred_at.isoformat() if mem.occurred_at else None,
            "score": round(score, 4),
            "connections": (mem.memory_metadata or {}).get("connections", {}).get("bubble_ids", [])
        })
    
    # Add connected
    for conn_mem in connected[:3]:
        results.append({
            "memory_id": conn_mem.id,
            "memory": conn_mem.memory_text,
            "type": "connected",
            "occurred_at": conn_mem.occurred_at.isoformat() if conn_mem.occurred_at else None,
            "score": 0,
            "connections": []
        })
    
    return {
        "query": query,
        "total": len(results),
        "results": results
    }
```

### Step 6: Update add_updation_phase.py

**File:** `src/contextmemory/memory/add/add_updation_phase.py`  
**Add FAISS indexing when memories are created**

**Add import at top:**
```python
from contextmemory.memory.vector_store import get_vector_store, save_vector_store
```

**Modify the ADD section (around line 42):**
```python
# ADD  
if tool_name == "add_memory":
    memory = Memory(
        conversation_id=conversation_id,
        memory_text=args["text"],
        embedding=fact_embedding,
        created_at=datetime.now(timezone.utc),
        updated_at=datetime.now(timezone.utc),
    )
    db.add(memory)
    db.flush()  # Get ID before adding to FAISS
    
    # Add to FAISS index
    vector_store = get_vector_store(conversation_id)
    vector_store.add(memory.id, fact_embedding)
```

**Add at the end of the function (before db.commit()):**
```python
# Save FAISS index
save_vector_store(conversation_id)

db.commit()
```

### Step 7: Update bubble_creator.py

**File:** `src/contextmemory/memory/bubble_creator.py`

**Add import at top:**
```python
from contextmemory.memory.vector_store import get_vector_store, save_vector_store
```

**After `db.flush()` in create_bubbles(), add:**
```python
db.flush()  # Get ID before finding connections

# Add to FAISS index
vector_store = get_vector_store(conversation_id)
vector_store.add(bubble.id, embedding)
```

**At the end of create_bubbles(), before `db.commit()` add:**
```python
# Save FAISS index
save_vector_store(conversation_id)

db.commit()
```

---

## 6. Complete Code Files

### File 1: vector_store.py (NEW)
Location: `src/contextmemory/memory/vector_store.py`

*(Full code provided in Step 3 above)*

### File 2: similar_memory_search.py (REPLACE)
Location: `src/contextmemory/memory/similar_memory_search.py`

*(Full code provided in Step 4 above)*

### File 3: memory.py search() (MODIFY)
Location: `src/contextmemory/memory/memory.py`

*(Full code provided in Step 5 above)*

### File 4: add_updation_phase.py (MODIFY)
Location: `src/contextmemory/memory/add/add_updation_phase.py`

*(Changes described in Step 6 above)*

### File 5: bubble_creator.py (MODIFY)
Location: `src/contextmemory/memory/bubble_creator.py`

*(Changes described in Step 7 above)*

---

## 7. Testing the Integration

### Quick Test

```bash
cd /Users/samikshashukla/Documents/context-memory
source venv/bin/activate
pip install faiss-cpu numpy
pip install -e .

# Run the test chatbot
python test_episodic.py
```

### Verify FAISS is Working

Add this to test timing:

```python
import time

# In chat_with_memories():
start = time.time()
search_results = memory.search(
    query=message,
    conversation_id=conversation_id,
    limit=5,
)
print(f"  ⚡ Search time: {(time.time() - start) * 1000:.2f}ms")
```

Expected: **<10ms** (vs 500ms+ before)

### Check Index Files

```bash
ls -la ~/.contextmemory/indexes/
# Should see:
# conv_1.faiss
# conv_1.map.json
```

---

## 8. Troubleshooting

### Issue: "No module named 'faiss'"
```bash
pip install faiss-cpu
```

### Issue: "Index is empty after restart"
The index auto-rebuilds from the database. If memories exist but index is empty:
```python
from contextmemory.memory.vector_store import rebuild_index_from_db
rebuild_index_from_db(db, conversation_id=1)
```

### Issue: "Dimension mismatch"
Ensure all embeddings are 1536 dimensions (OpenAI's text-embedding-3-small).

### Issue: "Memory not found after adding"
Make sure to call `save_vector_store(conversation_id)` after adding.

---

## Summary

| What | Why |
|------|-----|
| **FAISS** | ~100x faster similarity search |
| **IndexFlatIP** | Inner product = cosine similarity |
| **normalize_L2** | Required for cosine similarity |
| **Per-conversation indexes** | Isolation and smaller index sizes |
| **Auto-rebuild** | If index missing, rebuild from DB |

---

## Architecture After Integration

```
┌─────────────────────────────────────────────────────────────┐
│                    ContextMemory                            │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  search("query")                                            │
│       │                                                     │
│       ▼                                                     │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐     │
│  │ embed_text  │───▶│   FAISS     │───▶│  Database   │     │
│  │   (OpenAI)  │    │  (1-10ms)   │    │ (fetch by ID)│    │
│  └─────────────┘    └─────────────┘    └─────────────┘     │
│       ~200ms             ~5ms              ~10ms            │
│                                                             │
│  Total: ~215ms (vs ~1200ms before)                         │
└─────────────────────────────────────────────────────────────┘
```

---

**You're now ready to implement FAISS!** Follow Steps 1-7 in order, and use the test in Step 7 to verify it's working.
